{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "## Ramadharsh Vanchinathan\n",
    "## DATA 4319"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deep Neural Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a Neural Network?\n",
    "\n",
    "Neural Networks are the combination of artificial neurons that produce highly complex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/whatisnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological Neuron:\n",
    "\n",
    "* Dendrites receive and transmit signals to the nucleus\n",
    "\n",
    "* Those signals stimulate the nucleus and depending on the signal strength, the neuron will fire\n",
    "\n",
    "By imitating this process, we hope to imitate biological learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they work?\n",
    "\n",
    "* Each input signal(feature measure) is scaled by the first layer of weights, which affects the functino computed at that unit\n",
    "\n",
    "* The NN then computes a function of the input signal by propogating the computed values from the input signal to the output neurons and using weights as intermediate parameters.\n",
    "\n",
    "* Learning occurs by adjusting the weights in the NN in order to change the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "* We have a collection of $N$ label data points: $(x^1,y^1),...,(x^N,y^N)$.\n",
    "\n",
    "* Each datapoint contains 3 feature measures, i.e., $x^{(i)} \\in \\mathbb{R}^3$\n",
    "\n",
    "* for $i = 1,..,N$ each datapoint label indicates class A or class B\n",
    "\n",
    "    - Class A: $\\begin{bmatrix} 1\\\\0 \\end{bmatrix}$\n",
    "\n",
    "    - Class B: $\\begin{bmatrix} 0\\\\1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 0: Design Phase **\n",
    "\n",
    "---\n",
    "\n",
    "Build the NN Model.\n",
    "\n",
    "Weights and biases are initialized randomly(we will do this according to normal distribution)\n",
    "\n",
    "We will choose the number of layers to be 3(L = 3), one input layer, one hidden layer, one output layer.\n",
    "\n",
    "![](images/nnexample_model.jpg)\n",
    "\n",
    "$w^L_{i,j}$ := weight from neuron $j$ in layer $L - 1$ to neuron $i$ in layer $L$\n",
    "\n",
    "$b^L_i$ := bias connected to neuron $i$ in layer $L$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part I: Feedforward Phase **\n",
    "\n",
    "---\n",
    "\n",
    "Pass a given train feature $x^{(i)}$ into the input layer, along with 1.0 into each bias neuron.\n",
    "\n",
    "![](images/nn_ep1.jpg)\n",
    "\n",
    "Each feature measure is then transformed by a linear product with the layer 2 weights.\n",
    "\n",
    "![](images/nn_ep1_2.jpg)\n",
    "\n",
    "Each feature measure is then fransformed by linear product with layer 2 Weights.\n",
    "\n",
    "Pre-Activation General Formula: $z^L = W^L a^{L-1} + b^L$ for all layers $L$ from 2 to L.\n",
    "\n",
    "Post-Activation General Formula: $a^L = \\sigma(z^L)$\n",
    "\n",
    "![](images/nn_ep1_3.jpg)\n",
    "\n",
    "![](images/nn_ep1_4.jpg)\n",
    "\n",
    "![](images/nn_ep1_5.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_ep1_6.jpg)\n",
    "\n",
    "If our output $o = \\begin{bmatrix} .78 \\\\ .34 \\end{bmatrix}$, then we predict Class A.\n",
    "\n",
    "If our output $o = \\begin{bmatrix} .12 \\\\ .44 \\end{bmatrix}$, then we predict Class B.\n",
    "\n",
    "For the given input $x^{i}$ our NN is simply a function that maps $x^{i}$ to some output in $\\mathbb{R}^2$. Namely, the NN models a composition of functions that wishes to approximate the correct hypothesis for our desired output.\n",
    "\n",
    "Among all possible weights and biarses there exists a collection of fvalues that should approimate the correct hypothesis. Thus,w e train the weights and biases to be the best fitting function according to some cost function(using some optimization technique).\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part II: Backpropagation **\n",
    "\n",
    "---\n",
    "\n",
    "The Loss Function\n",
    "\n",
    "$C(W,b) = \\frac{1}{2} \\sum^n_{k=1} (a^L_k - y^(i)_k)^2 = \\frac{1}{2} (a^3_1 - y^{i}_1)^2 + \\frac{1}{2} (a^3_2 - y^{i}_2)^2 $\n",
    "\n",
    "Find the gradient of $C(W,b)$ with respect to $W$ and $b$\n",
    "\n",
    "$W^{k+1} = W^k - \\alpha \\frac{\\partial C}{\\partial W^k}$\n",
    "\n",
    "$b^{k+1} = b^k - \\alpha \\frac{\\partial C}{\\partial b^k}$\n",
    "\n",
    "## Output Error: How much does one weight($w^3_{1,1}$ for example) affect the loss Function\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^3_{1,1}}$\n",
    "\n",
    "$= \\frac{\\partial}{\\partial w^3_{1,1}} C(W,b)$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\frac{\\partial}{\\partial w^3_{1,1}} [a^3_1]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\frac{\\partial}{\\partial w^3_{1,1}} [\\sigma (z^3_1)]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\sigma'(z^3_1) \\frac{\\partial}{\\partial w^3_{1,1}} [z^3_1]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\sigma'(z^3_1) \\frac{\\partial}{\\partial w^3_{1,1}} [a^2_1 w^3_{1,1} + a^2_2 w^3_{1,2} + b^3_1]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\sigma'(z^3_1) a^2_1$\n",
    "\n",
    "Output Error of $w^3_{1,1}$: $ \\delta^3_1 = (a^3_1 - y^{(i)}_1) \\sigma'(z^3_1) = \\frac{\\partial C}{\\partial b^3_1}$\n",
    "\n",
    "**General Output Error Formula**\n",
    "\n",
    "$\\delta ^\\ell_j = \\frac{\\partial C}{\\partial a^\\ell_j}\\sigma'(z^\\ell_j)$\n",
    "\n",
    "$\\delta^\\ell = \\nabla_aC \\otimes \\sigma'(z^\\ell) = \\begin{bmatrix} \\frac{\\partial C}{\\partial a^\\ell_1} = (a^\\ell_1 - y^{(i)}_1) \\\\ \\frac{\\partial C}{\\partial a^\\ell_2} = (a^\\ell_2 - y^{(i)}_2)\\\\\\ ... \\\\ \\frac{\\partial C}{\\partial a^\\ell_n} = (a^\\ell_n - y^{(i)}_n)\\end{bmatrix} \\otimes \\begin{bmatrix} \\sigma'(z^\\ell_1) \\\\ \\sigma'(z^\\ell_2) \\\\ ... \\\\ \\sigma'(z^\\ell_n)  \\end{bmatrix} = \\begin{bmatrix} (a^\\ell_1 - y^{(i)}_1) \\sigma'(z^\\ell_1) \\\\ (a^\\ell_2 - y^{(i)}_2) \\sigma'(z^\\ell_2) \\\\ ... \\\\ (a^\\ell_n - y^{(i)}_n) \\sigma'(z^\\ell_n) \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Error\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^2_{1,1}} = \\frac{\\partial}{\\partial w^2_{1,1}} C(W,b)$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\frac{\\partial}{\\partial w^2_{1,1}} [a^3_1] + (a^3_2 - y^{(i)}_2) \\frac{\\partial}{\\partial w^2_{1,1}} [a^3_2]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\frac{\\partial}{\\partial w^2_{1,1}} [\\sigma(z^3_1)] + (a^3_2 - y^{(i)}_2) \\frac{\\partial}{\\partial w^2_{1,1}} [\\sigma(z^3_2]$\n",
    "\n",
    "$= (a^3_1 - y^{(i)}_1) \\sigma'(z^3_1) \\frac{\\partial}{\\partial w^2_{1,1}} [z^3_1] + (a^3_1 - y^{(i)}_2) \\sigma'(z^3_2) \\frac{\\partial}{\\partial w^2_{1,1}} [z^3_2]$\n",
    "\n",
    "$= \\delta^3_1 \\frac{\\partial}{\\partial w^2_{1,1}}[a^2_1w^3_{1,1} + a^2_2w^3_{1,2} + b^3 _1] + \\delta^3_2 \\frac{\\partial}{\\partial w^2_{1,1}} [a^2_1w^3_{2,1} + a^2_2w^3_{2,2} + b^3 _2]$\n",
    "\n",
    "$= \\delta^3_1w^3_{1,1}\\frac{\\partial}{\\partial w^2_{1,1}}[\\sigma(z^2_1)] + \\delta^3_2w^2_{1,1}[\\sigma(z^2_1)]$\n",
    "\n",
    "$= \\begin{bmatrix} w^3_{1,1} \\\\ w^3_{2,1} \\end{bmatrix}^T \\begin{bmatrix} \\delta^3_1 \\\\ \\delta^3_2 \\end{bmatrix} \\sigma'(z^2_1)\\frac{\\partial}{\\partial w^2_{1,1}} [w^2_{1,1}a^1_1 + w^2_{1,2}a^1_2 + w^2_{1,3}a^1_3 + b^2_1]$\n",
    "\n",
    "$= \\begin{bmatrix} w^3_{1,1} \\\\ w^3_{2,1} \\end{bmatrix}^T \\begin{bmatrix} \\delta^3_1 \\\\ \\delta^3_2 \\end{bmatrix} \\sigma'(z^2_1) a^1_1$\n",
    "\n",
    "Neuron Error of $w^2_{1,1}$\n",
    "\n",
    "**General Neuron Error Formula**\n",
    "\n",
    "$\\sigma^\\ell = ((w^{\\ell+1})^T\\delta^{\\ell+1}) \\otimes \\sigma'(z^\\ell)$\n",
    "\n",
    "for $\\ell = L-1,...,2$\n",
    "\n",
    "The Partial Derivatives\n",
    "\n",
    "$\\frac{\\partial C}{\\partial x^\\ell_{j,k}} = a^{\\ell - 1}_k\\delta^\\ell_j$\n",
    "\n",
    "$\\frac{\\partial C}\\partial b^\\ell_j = \\delta^\\ell_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropogation Algorithm**\n",
    "\n",
    "1. Input $x^{(i)}$: Set the corresponding activation $a^1$ for the input layer.\n",
    "\n",
    "1. Feedforward: For each $\\ell = 2,...,L$ compute $z^\\ell = w^\\ell a^{\\ell-1}+b^\\ell$ and $a^\\ell = \\sigma(z^\\ell)$\n",
    "\n",
    "1. Output error $\\sigma^L$: Compute the vector $\\delta = \\nabla_aC \\otimes \\sigma'(z^L)$\n",
    "\n",
    "1. Backpropogate the error: For each $\\ell = L-1,L-2,...,2$ compute $((w^{\\ell+1})^T\\delta^{\\ell+1}) \\otimes \\sigma'(z^\\ell)$\n",
    "\n",
    "1. Output: The gradient of the cost function is given by:\n",
    "\n",
    "    - $\\frac{\\partial C}{\\partial x^\\ell_{j,k}} = a^{\\ell - 1}_k\\delta^\\ell_j$\n",
    "\n",
    "    - $\\frac{\\partial C}\\partial b^\\ell_j = \\delta^\\ell_j$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
